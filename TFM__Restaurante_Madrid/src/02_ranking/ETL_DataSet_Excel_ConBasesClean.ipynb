{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7c0ca6-d468-4a57-afa0-21be40f0c5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\geith\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: geopandas in c:\\users\\geith\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: shapely in c:\\users\\geith\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: pyproj in c:\\users\\geith\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\geith\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\geith\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\geith\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\geith\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\geith\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in c:\\users\\geith\\anaconda3\\lib\\site-packages (from geopandas) (0.11.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\geith\\anaconda3\\lib\\site-packages (from geopandas) (24.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\geith\\anaconda3\\lib\\site-packages (from pyproj) (2025.4.26)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\geith\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\geith\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas geopandas shapely pyproj numpy openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb892d9-22cf-4060-91e7-87acee1f64c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: C:\\Users\\geith\\TFM INESDI\n",
      "OUT_CSV: C:\\Users\\geith\\TFM INESDI\\ML_RestaurantesMadrid_Features_Final_V4_3.csv\n",
      "âœ… Bases cargadas: {'B1': (19325, 21), 'B2': (82, 8), 'B3': (6298, 19), 'B4': (128833, 8), 'B5': (59752, 19), 'B6': (131, 7), 'B7': (1064, 10), 'B8': (53, 15), 'B9': (250, 6)}\n",
      "âœ… Capas GEO: {'B1': 19325, 'B3': 6298, 'B4': 128833, 'B5': 59752, 'B8': 53, 'B9': 250}\n",
      "ðŸ§± Grid creado: 39,858 celdas.\n",
      "âš™ Features baseâ€¦\n",
      "âš™ Intermodal scoreâ€¦\n",
      "âš™ Demanda residente (B7)â€¦\n",
      "âš™ Mex affinity (B6) â€” armonizando cod_barrioâ€¦\n",
      "Matches B6â†”Barrios: 131 / 131\n",
      "âš™ Presencia MX (B2)â€¦\n",
      "\n",
      "--- SANITY CHECKS ---\n",
      "footfall_day_avg describe:\n",
      " count    5569.000000\n",
      "mean      342.931693\n",
      "std       396.052182\n",
      "min         1.600000\n",
      "25%        29.000000\n",
      "50%       235.400000\n",
      "75%       490.000000\n",
      "max      6307.600000\n",
      "Name: footfall_day_avg, dtype: float64\n",
      "intermodal_score describe:\n",
      " count    39858.000000\n",
      "mean         0.478252\n",
      "std          0.139542\n",
      "min          0.000000\n",
      "25%          0.396904\n",
      "50%          0.503462\n",
      "75%          0.582150\n",
      "max          0.923271\n",
      "Name: intermodal_score, dtype: float64\n",
      "mex_affinity_proxy describe:\n",
      " count    39858.000000\n",
      "mean         0.166753\n",
      "std          0.173035\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.088915\n",
      "75%          0.286025\n",
      "max          0.639140\n",
      "Name: mex_affinity_proxy, dtype: float64\n",
      "mexican_location_score describe:\n",
      " count    39858.000000\n",
      "mean         0.380176\n",
      "std          0.080925\n",
      "min          0.200000\n",
      "25%          0.315888\n",
      "50%          0.359817\n",
      "75%          0.447918\n",
      "max          0.758014\n",
      "Name: mexican_location_score, dtype: float64\n",
      "\n",
      "âœ… ETL V4.3 exportado â†’ C:\\Users\\geith\\TFM INESDI\\ML_RestaurantesMadrid_Features_Final_V4_3.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================\n",
    "# ETL RESTAURANTES MADRID â€” V4.3 (LOCAL JUPYTERLAB, BASES LIMPIAS)\n",
    "#   - B7 actualizado a _LIMPIO_v2 (sin filas \"Todos\")\n",
    "#   - FIX DEFINITIVO B6: armoniza cod_barrio (B6 ordinal) vs COD_BAR (SHP)\n",
    "#       => cod_barrio_city = cod_distrito*10 + cod_barrio (cuando aplica)\n",
    "#   - Footfall con sjoin_nearest (rÃ¡pido)\n",
    "#   - Competencia cruda guardada (competition_raw)\n",
    "# ==============================================================\n",
    "\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# CONFIG\n",
    "# --------------------------------------------------------------\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = BASE_DIR\n",
    "PATH_BARRIOS_SHP = os.path.join(BASE_DIR, \"Barrios\", \"BARRIOS.shp\")\n",
    "\n",
    "OUT_CSV = os.path.join(BASE_DIR, \"ML_RestaurantesMadrid_Features_Final_V4_3.csv\")\n",
    "\n",
    "CELL_SIZE_M = 150\n",
    "BUFFER_250  = 250\n",
    "BUFFER_500  = 500\n",
    "\n",
    "CRS_WGS84  = \"EPSG:4326\"\n",
    "CRS_METRIC = \"EPSG:25830\"\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"OUT_CSV:\", OUT_CSV)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# HELPERS\n",
    "# --------------------------------------------------------------\n",
    "def read_any(path):\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xls\"]:\n",
    "        return pd.read_excel(path)\n",
    "    elif ext in [\".csv\", \".txt\"]:\n",
    "        # nota: si tu CSV trae BOM (como B4), pandas suele leer bien igual\n",
    "        return pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Formato no soportado: {ext}\")\n",
    "\n",
    "def pick_col(df, options):\n",
    "    for c in options:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_minmax(s):\n",
    "    if not hasattr(s, \"dropna\"):\n",
    "        s = pd.Series([s])\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s.dropna().empty or s.max() == s.min():\n",
    "        return s*0\n",
    "    return (s - s.min())/(s.max() - s.min())\n",
    "\n",
    "def build_square_grid_safe(points_list,\n",
    "                           cell_size=CELL_SIZE_M,\n",
    "                           pad=800,\n",
    "                           max_cells=300_000,\n",
    "                           guardrail=(350000,4300000,700000,4900000)):\n",
    "\n",
    "    valid_layers = [g for g in points_list if g is not None and len(g)]\n",
    "    bounds = [g.total_bounds for g in valid_layers if np.isfinite(g.total_bounds).all()]\n",
    "    if not bounds:\n",
    "        raise ValueError(\"No hay capas vÃ¡lidas para calcular grid.\")\n",
    "\n",
    "    arr = np.array(bounds)\n",
    "    q = lambda a: (np.nanquantile(a,0.01), np.nanquantile(a,0.99))\n",
    "\n",
    "    xmin = q(arr[:,0])[0] - pad\n",
    "    ymin = q(arr[:,1])[0] - pad\n",
    "    xmax = q(arr[:,2])[1] + pad\n",
    "    ymax = q(arr[:,3])[1] + pad\n",
    "\n",
    "    gxmin,gymin,gxmax,gymax = guardrail\n",
    "    xmin=max(xmin,gxmin); ymin=max(ymin,gymin)\n",
    "    xmax=min(xmax,gxmax); ymax=min(ymax,gymax)\n",
    "\n",
    "    width, height = xmax-xmin, ymax-ymin\n",
    "    nx, ny = int(np.ceil(width/cell_size)), int(np.ceil(height/cell_size))\n",
    "    total = nx*ny\n",
    "\n",
    "    if total > max_cells:\n",
    "        new_size = int(np.ceil(np.sqrt((width*height)/max_cells)))\n",
    "        print(f\"âš  Grid enorme ({total:,}). Ajustando cell_size â†’ {new_size} m.\")\n",
    "        cell_size=new_size\n",
    "\n",
    "    xs = np.arange(xmin, xmax, cell_size)\n",
    "    ys = np.arange(ymin, ymax, cell_size)\n",
    "    cells = [box(x,y,x+cell_size,y+cell_size) for x in xs for y in ys]\n",
    "\n",
    "    grid = gpd.GeoDataFrame({\"geometry\":cells}, crs=CRS_METRIC)\n",
    "    grid[\"grid_id\"] = [f\"cell_{i:06d}\" for i in range(len(grid))]\n",
    "\n",
    "    cent = grid.geometry.centroid\n",
    "    grid[\"lat_center\"] = cent.to_crs(CRS_WGS84).y\n",
    "    grid[\"lon_center\"] = cent.to_crs(CRS_WGS84).x\n",
    "\n",
    "    print(f\"ðŸ§± Grid creado: {len(grid):,} celdas.\")\n",
    "    return grid\n",
    "\n",
    "def to_gdf_latlon(df, lat_col, lon_col, key=\"\"):\n",
    "    tmp = df.copy()\n",
    "    tmp[lat_col] = pd.to_numeric(tmp[lat_col], errors=\"coerce\")\n",
    "    tmp[lon_col] = pd.to_numeric(tmp[lon_col], errors=\"coerce\")\n",
    "    tmp = tmp.dropna(subset=[lat_col, lon_col])\n",
    "    if tmp.empty:\n",
    "        raise ValueError(f\"{key}: sin coords vÃ¡lidas en {lat_col}/{lon_col}\")\n",
    "    gdf = gpd.GeoDataFrame(tmp, geometry=gpd.points_from_xy(tmp[lon_col], tmp[lat_col]), crs=CRS_WGS84)\n",
    "    return gdf.to_crs(CRS_METRIC)\n",
    "\n",
    "def to_gdf_xy(df, x_col, y_col, key=\"\"):\n",
    "    tmp = df.copy()\n",
    "    tmp[x_col] = pd.to_numeric(tmp[x_col], errors=\"coerce\")\n",
    "    tmp[y_col] = pd.to_numeric(tmp[y_col], errors=\"coerce\")\n",
    "    tmp = tmp.dropna(subset=[x_col, y_col])\n",
    "    if tmp.empty:\n",
    "        raise ValueError(f\"{key}: sin coords vÃ¡lidas en {x_col}/{y_col}\")\n",
    "    gdf = gpd.GeoDataFrame(tmp, geometry=gpd.points_from_xy(tmp[x_col], tmp[y_col]), crs=CRS_METRIC)\n",
    "    return gdf\n",
    "\n",
    "def count_points_within(gdf_points, centers, radius_m):\n",
    "    if gdf_points is None or len(gdf_points)==0:\n",
    "        return pd.Series(0, index=centers.index)\n",
    "    return centers.apply(lambda c: gdf_points.within(c.buffer(radius_m)).sum())\n",
    "\n",
    "def min_distance_to(gdf_points, centers):\n",
    "    if gdf_points is None or len(gdf_points)==0:\n",
    "        return pd.Series(np.nan, index=centers.index)\n",
    "    return centers.apply(lambda c: gdf_points.distance(c).min())\n",
    "\n",
    "def sum_points_within(gdf_points, centers, radius_m, value_col):\n",
    "    if gdf_points is None or len(gdf_points)==0 or (value_col is None) or (value_col not in gdf_points.columns):\n",
    "        return pd.Series(0.0, index=centers.index)\n",
    "    return centers.apply(lambda c: gdf_points.loc[gdf_points.within(c.buffer(radius_m)), value_col].sum())\n",
    "\n",
    "def harmonize_cod_barrio(df, col_d=\"cod_distrito\", col_b=\"cod_barrio\"):\n",
    "    \"\"\"\n",
    "    Armoniza cod_barrio:\n",
    "    - Si parece \"ordinal\" (1..9, o tÃ­picamente < 11), lo convierte a \"cÃ³digo ciudad\" = distrito*10 + barrio\n",
    "    - Si ya parece \"cÃ³digo ciudad\" (>= 11, 3 dÃ­gitos, etc.), lo deja igual.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[col_d] = pd.to_numeric(out[col_d], errors=\"coerce\")\n",
    "    out[col_b] = pd.to_numeric(out[col_b], errors=\"coerce\")\n",
    "    # criterio prÃ¡ctico: si el mÃ¡ximo de cod_barrio es <= 10, casi seguro es ordinal\n",
    "    if pd.to_numeric(out[col_b], errors=\"coerce\").max(skipna=True) <= 10:\n",
    "        out[col_b] = (out[col_d] * 10 + out[col_b])\n",
    "    return out\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# CARGA BASES LIMPIAS\n",
    "# --------------------------------------------------------------\n",
    "paths = {\n",
    "    \"B1\": os.path.join(DATA_DIR,\"B1_Restaurantes_2025_Lat_Long_CLEAN.csv\"),\n",
    "    \"B2\": os.path.join(DATA_DIR,\"B2_Menu_RestaurantesMX_LIMPIO.csv\"),\n",
    "    \"B3\": os.path.join(DATA_DIR,\"B3_Restaurantes_Terrazas_2025_Lat_Long_LIMPIO.csv\"),\n",
    "    \"B4\": os.path.join(DATA_DIR,\"B4_Flujo_Peatones_2024_LIMPIO.csv\"),\n",
    "    \"B5\": os.path.join(DATA_DIR,\"B5_Licencias_2025_Lat_Long_LIMPIO.csv\"),\n",
    "    \"B6\": os.path.join(DATA_DIR,\"B6_Residentes_Edad_Nacionalidad_2025_LIMPIO.csv\"),\n",
    "    \"B7\": os.path.join(DATA_DIR,\"B7_Poblacion_Madrid_LIMPIO_v2.csv\"),  # âœ… v2\n",
    "    \"B8\": os.path.join(DATA_DIR,\"B8_Aparcamientos_Publicos.xlsx\"),\n",
    "    \"B9\": os.path.join(DATA_DIR,\"B9_Estaciones_Metro_Renfe_CLEAN.csv\"),\n",
    "}\n",
    "\n",
    "missing = [(k,p) for k,p in paths.items() if not os.path.exists(p)]\n",
    "if missing:\n",
    "    print(\"âŒ FALTAN ARCHIVOS:\")\n",
    "    for k,p in missing:\n",
    "        print(\"  -\", k, \"->\", p)\n",
    "    raise FileNotFoundError(\"Corrige los nombres/rutas de los archivos faltantes y reintenta.\")\n",
    "\n",
    "dfs = {k: read_any(p) for k,p in paths.items()}\n",
    "gdf_barrios = gpd.read_file(PATH_BARRIOS_SHP)\n",
    "\n",
    "print(\"âœ… Bases cargadas:\", {k: dfs[k].shape for k in dfs})\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# GEO CAPAS\n",
    "# --------------------------------------------------------------\n",
    "gdfs = {}\n",
    "\n",
    "# --- B1 ---\n",
    "dfB1 = dfs[\"B1\"].copy()\n",
    "xB1  = pick_col(dfB1, [\"x_num\",\"coordenada_x_local\",\"COORDENADA-X\",\"X\",\"x\"])\n",
    "yB1  = pick_col(dfB1, [\"y_num\",\"coordenada_y_local\",\"COORDENADA-Y\",\"Y\",\"y\"])\n",
    "latB1= pick_col(dfB1, [\"Latitud\",\"lat\",\"latitude\",\"lat_num\",\"Latitud_num\"])\n",
    "lonB1= pick_col(dfB1, [\"Longitud\",\"longitud\",\"lon\",\"longitude\",\"lon_num\",\"Longitud_num\",\"longitud_num\"])\n",
    "\n",
    "if xB1 and yB1:\n",
    "    gdfs[\"B1\"] = to_gdf_xy(dfB1, xB1, yB1, \"B1\")\n",
    "elif latB1 and lonB1:\n",
    "    gdfs[\"B1\"] = to_gdf_latlon(dfB1, latB1, lonB1, \"B1\")\n",
    "else:\n",
    "    raise ValueError(f\"B1: no encuentro coords. Columnas: {list(dfB1.columns)}\")\n",
    "\n",
    "# --- B3 (terrazas) ---\n",
    "dfB3 = dfs[\"B3\"].copy()\n",
    "xB3 = pick_col(dfB3, [\"coordenada_x_local\",\"x_num\",\"COORDENADA-X\"])\n",
    "yB3 = pick_col(dfB3, [\"coordenada_y_local\",\"y_num\",\"COORDENADA-Y\"])\n",
    "if xB3 and yB3:\n",
    "    gdfs[\"B3\"] = to_gdf_xy(dfB3, xB3, yB3, \"B3\")\n",
    "else:\n",
    "    latB3 = pick_col(dfB3, [\"Latitud\",\"lat\"])\n",
    "    lonB3 = pick_col(dfB3, [\"Longitud\",\"longitud\",\"lon\"])\n",
    "    gdfs[\"B3\"] = to_gdf_latlon(dfB3, latB3, lonB3, \"B3\")\n",
    "\n",
    "# --- B4 (peatones) ---\n",
    "dfB4 = dfs[\"B4\"].copy()\n",
    "lat4 = pick_col(dfB4, [\"Latitud_num\",\"Latitud\",\"lat\",\"latitude\"])\n",
    "lon4 = pick_col(dfB4, [\"Longitud_num\",\"longitud_num\",\"Longitud\",\"longitud\",\"lon\",\"longitude\"])\n",
    "if lat4 is None or lon4 is None:\n",
    "    raise ValueError(f\"B4: no encuentro lat/lon. Columnas: {list(dfB4.columns)}\")\n",
    "gdfs[\"B4\"] = to_gdf_latlon(dfB4, lat4, lon4, \"B4\")\n",
    "\n",
    "# --- B5 (licencias) ---\n",
    "dfB5 = dfs[\"B5\"].copy()\n",
    "xB5 = pick_col(dfB5, [\"x_num\",\"coordenada_x_local\",\"COORDENADA-X\"])\n",
    "yB5 = pick_col(dfB5, [\"y_num\",\"coordenada_y_local\",\"COORDENADA-Y\"])\n",
    "if xB5 and yB5:\n",
    "    gdfs[\"B5\"] = to_gdf_xy(dfB5, xB5, yB5, \"B5\")\n",
    "else:\n",
    "    latB5 = pick_col(dfB5, [\"Latitud\",\"lat\"])\n",
    "    lonB5 = pick_col(dfB5, [\"Longitud\",\"longitud\",\"lon\"])\n",
    "    gdfs[\"B5\"] = to_gdf_latlon(dfB5, latB5, lonB5, \"B5\")\n",
    "\n",
    "# --- B8 (parkings) ---\n",
    "dfB8 = dfs[\"B8\"].copy()\n",
    "xB8 = pick_col(dfB8, [\"COORDENADA-X\",\"coordenada_x_local\",\"x_num\",\"X\"])\n",
    "yB8 = pick_col(dfB8, [\"COORDENADA-Y\",\"coordenada_y_local\",\"y_num\",\"Y\"])\n",
    "if xB8 and yB8:\n",
    "    gdfs[\"B8\"] = to_gdf_xy(dfB8, xB8, yB8, \"B8\")\n",
    "else:\n",
    "    latB8 = pick_col(dfB8, [\"LATITUD\",\"Latitud\",\"lat\"])\n",
    "    lonB8 = pick_col(dfB8, [\"LONGITUD\",\"Longitud\",\"longitud\",\"lon\"])\n",
    "    gdfs[\"B8\"] = to_gdf_latlon(dfB8, latB8, lonB8, \"B8\")\n",
    "\n",
    "# --- B9 (metro/renfe) ---\n",
    "dfB9 = dfs[\"B9\"].copy()\n",
    "latB9 = pick_col(dfB9, [\"Latitud\",\"lat\"])\n",
    "lonB9 = pick_col(dfB9, [\"Longitud\",\"longitud\",\"lon\"])\n",
    "gdfs[\"B9\"] = to_gdf_latlon(dfB9, latB9, lonB9, \"B9\")\n",
    "\n",
    "print(\"âœ… Capas GEO:\", {k: len(gdfs[k]) for k in gdfs})\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# GRID\n",
    "# --------------------------------------------------------------\n",
    "grid = build_square_grid_safe([gdfs[\"B1\"], gdfs[\"B5\"], gdfs[\"B8\"], gdfs[\"B9\"], gdfs[\"B4\"]])\n",
    "cent = grid.geometry.centroid\n",
    "cent_gdf = gpd.GeoDataFrame(grid[[\"grid_id\"]].copy(), geometry=cent, crs=CRS_METRIC)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# FEATURES BASE\n",
    "# --------------------------------------------------------------\n",
    "print(\"âš™ Features baseâ€¦\")\n",
    "\n",
    "grid[\"rest_total_250m\"] = count_points_within(gdfs[\"B1\"], cent, BUFFER_250)\n",
    "grid[\"rest_total_500m\"] = count_points_within(gdfs[\"B1\"], cent, BUFFER_500)\n",
    "grid[\"rest_density_500m\"] = grid[\"rest_total_500m\"] / (np.pi * (BUFFER_500**2))\n",
    "\n",
    "# competencia cruda (para ML)\n",
    "grid[\"competition_raw\"] = grid[\"rest_density_500m\"]\n",
    "\n",
    "grid[\"dist_metro_min\"] = min_distance_to(gdfs[\"B9\"], cent)\n",
    "\n",
    "# Footfall (nearest 5)\n",
    "foot_col = pick_col(gdfs[\"B4\"], [\"cantidad_peatones_num\",\"cantidad_peatones\"])\n",
    "if foot_col is None:\n",
    "    grid[\"footfall_day_avg\"] = np.nan\n",
    "else:\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        cent_gdf,\n",
    "        gdfs[\"B4\"][[foot_col, \"geometry\"]].copy(),\n",
    "        how=\"left\",\n",
    "        distance_col=\"dist\",\n",
    "        max_distance=2000\n",
    "    )\n",
    "    nearest = nearest.sort_values([\"grid_id\",\"dist\"]).groupby(\"grid_id\").head(5)\n",
    "    foot = nearest.groupby(\"grid_id\")[foot_col].mean().rename(\"footfall_day_avg\")\n",
    "    grid = grid.merge(foot, on=\"grid_id\", how=\"left\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# INTERMODAL SCORE (metro + parking)\n",
    "# --------------------------------------------------------------\n",
    "print(\"âš™ Intermodal scoreâ€¦\")\n",
    "\n",
    "grid[\"estaciones_500m\"] = count_points_within(gdfs[\"B9\"], cent, 500)\n",
    "grid[\"dist_parking_min\"] = min_distance_to(gdfs[\"B8\"], cent)\n",
    "\n",
    "cap_col = pick_col(gdfs[\"B8\"], [\"Plazas_Publicas\",\"PlazasPublicas\",\"PLAZAS_PUBLICAS\",\"plazas_publicas\",\"plazas\"])\n",
    "grid[\"plazas_parking_500m\"] = (\n",
    "    sum_points_within(gdfs[\"B8\"], cent, 500, cap_col) if cap_col else count_points_within(gdfs[\"B8\"], cent, 500)\n",
    ")\n",
    "\n",
    "dist_metro_norm   = 1 - normalize_minmax(grid[\"dist_metro_min\"])\n",
    "dist_parking_norm = 1 - normalize_minmax(grid[\"dist_parking_min\"])\n",
    "estaciones_norm   = normalize_minmax(grid[\"estaciones_500m\"])\n",
    "plazas_norm       = normalize_minmax(grid[\"plazas_parking_500m\"])\n",
    "\n",
    "grid[\"intermodal_score\"] = (\n",
    "    0.45*dist_metro_norm +\n",
    "    0.15*estaciones_norm +\n",
    "    0.40*((plazas_norm + dist_parking_norm)/2.0)\n",
    ").fillna(0).clip(0,1)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# SHP NORMALIZADO (base para B7 y B6)\n",
    "# --------------------------------------------------------------\n",
    "gdf_barrios_25830 = gdf_barrios.to_crs(CRS_METRIC).copy()\n",
    "\n",
    "gdf_barrios_25830[\"cod_distrito\"] = pd.to_numeric(\n",
    "    gdf_barrios_25830[\"CODDIS\"].astype(str).str.strip(),\n",
    "    errors=\"coerce\"\n",
    ").astype(\"Int64\")\n",
    "\n",
    "# COD_BAR viene como '011' -> 11\n",
    "gdf_barrios_25830[\"cod_barrio\"] = pd.to_numeric(\n",
    "    gdf_barrios_25830[\"COD_BAR\"].astype(str).str.strip().str.lstrip(\"0\").replace(\"\", \"0\"),\n",
    "    errors=\"coerce\"\n",
    ").astype(\"Int64\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# DEMANDA RESIDENTE (B7 v2) + BARRIOS\n",
    "# --------------------------------------------------------------\n",
    "print(\"âš™ Demanda residente (B7)â€¦\")\n",
    "\n",
    "df7 = dfs[\"B7\"].copy()\n",
    "pop_col = pick_col(df7, [\"num_personas_num\",\"num_personas\"])\n",
    "if pop_col is None:\n",
    "    raise ValueError(f\"B7: no encuentro columna poblaciÃ³n. Columnas: {list(df7.columns)}\")\n",
    "\n",
    "df7[\"cod_distrito\"] = pd.to_numeric(df7[\"cod_distrito\"], errors=\"coerce\")\n",
    "df7[\"cod_barrio\"]   = pd.to_numeric(df7[\"cod_barrio\"], errors=\"coerce\")\n",
    "df7[pop_col]        = pd.to_numeric(df7[pop_col], errors=\"coerce\")\n",
    "\n",
    "# ðŸ”§ armoniza si B7 trae ordinal (1..9)\n",
    "df7 = harmonize_cod_barrio(df7, \"cod_distrito\", \"cod_barrio\")\n",
    "\n",
    "df7 = df7.dropna(subset=[\"cod_distrito\",\"cod_barrio\"])\n",
    "df7[\"cod_distrito\"] = df7[\"cod_distrito\"].astype(int)\n",
    "df7[\"cod_barrio\"]   = df7[\"cod_barrio\"].astype(int)\n",
    "\n",
    "df7_agg = df7.groupby([\"cod_distrito\",\"cod_barrio\"], as_index=False)[pop_col].sum()\n",
    "df7_agg = df7_agg.rename(columns={pop_col:\"num_personas\"})\n",
    "\n",
    "gdf_dem = gdf_barrios_25830.merge(df7_agg, on=[\"cod_distrito\",\"cod_barrio\"], how=\"left\")\n",
    "gdf_dem[\"num_personas\"] = gdf_dem[\"num_personas\"].fillna(0)\n",
    "\n",
    "grid_barr = gpd.sjoin(\n",
    "    cent_gdf,\n",
    "    gdf_dem[[\"cod_distrito\",\"cod_barrio\",\"num_personas\",\"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"\n",
    ")\n",
    "\n",
    "grid = grid.merge(\n",
    "    grid_barr[[\"grid_id\",\"cod_distrito\",\"cod_barrio\",\"num_personas\"]],\n",
    "    on=\"grid_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "grid[\"num_personas\"] = grid[\"num_personas\"].fillna(0)\n",
    "grid[\"res_density\"]  = normalize_minmax(grid[\"num_personas\"]).fillna(0)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# B6 LIMPIO â€” Mex affinity (FIX REAL: convertir barrio ordinal a cÃ³digo ciudad)\n",
    "# --------------------------------------------------------------\n",
    "print(\"âš™ Mex affinity (B6) â€” armonizando cod_barrioâ€¦\")\n",
    "\n",
    "df6 = dfs[\"B6\"].copy()\n",
    "df6[\"cod_distrito\"] = pd.to_numeric(df6[\"cod_distrito\"], errors=\"coerce\")\n",
    "df6[\"cod_barrio\"]   = pd.to_numeric(df6[\"cod_barrio\"], errors=\"coerce\")\n",
    "df6[\"mex_affinity_proxy\"] = pd.to_numeric(df6[\"mex_affinity_proxy\"], errors=\"coerce\")\n",
    "\n",
    "# ðŸ”§ si B6 trae ordinal (1..9) => cod_barrio = distrito*10 + barrio\n",
    "df6 = harmonize_cod_barrio(df6, \"cod_distrito\", \"cod_barrio\")\n",
    "\n",
    "df6[\"cod_distrito\"] = df6[\"cod_distrito\"].astype(\"Int64\")\n",
    "df6[\"cod_barrio\"]   = df6[\"cod_barrio\"].astype(\"Int64\")\n",
    "\n",
    "# Merge B6 sobre geometrÃ­a barrios\n",
    "gdf_b6 = gdf_barrios_25830.merge(\n",
    "    df6[[\"cod_distrito\",\"cod_barrio\",\"mex_affinity_proxy\"]],\n",
    "    on=[\"cod_distrito\",\"cod_barrio\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "matches = gdf_b6[\"mex_affinity_proxy\"].notna().sum()\n",
    "print(\"Matches B6â†”Barrios:\", matches, \"/\", len(gdf_b6))\n",
    "\n",
    "gdf_b6[\"mex_affinity_proxy\"] = gdf_b6[\"mex_affinity_proxy\"].fillna(0)\n",
    "\n",
    "# Spatial join a centros de grid\n",
    "grid_b6 = gpd.sjoin(\n",
    "    cent_gdf,\n",
    "    gdf_b6[[\"mex_affinity_proxy\",\"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"\n",
    ")\n",
    "\n",
    "grid = grid.merge(grid_b6[[\"grid_id\",\"mex_affinity_proxy\"]], on=\"grid_id\", how=\"left\")\n",
    "grid[\"mex_affinity_proxy\"] = grid[\"mex_affinity_proxy\"].fillna(0)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# B2 LIMPIO: presencia MX (mapea id_local de B2 contra B1)\n",
    "# --------------------------------------------------------------\n",
    "print(\"âš™ Presencia MX (B2)â€¦\")\n",
    "\n",
    "df2 = dfs[\"B2\"].copy()\n",
    "\n",
    "grid[\"mx_rest_total_500m\"] = 0\n",
    "if \"id_local\" in df2.columns:\n",
    "    mx_ids = set(pd.to_numeric(df2[\"id_local\"], errors=\"coerce\").dropna().astype(int).tolist())\n",
    "    idcol_b1 = pick_col(gdfs[\"B1\"], [\"id_local\",\"id\",\"ID_LOCAL\"])\n",
    "    if idcol_b1:\n",
    "        b1_ids = pd.to_numeric(gdfs[\"B1\"][idcol_b1], errors=\"coerce\")\n",
    "        gdf_mx = gdfs[\"B1\"].loc[b1_ids.isin(mx_ids)].copy()\n",
    "        grid[\"mx_rest_total_500m\"] = count_points_within(gdf_mx, cent, 500)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# SCORE / OUTPUTS\n",
    "# --------------------------------------------------------------\n",
    "ratio = grid[\"rest_total_250m\"] / (grid[\"rest_total_500m\"].replace(0, np.nan))\n",
    "ratio = ratio.fillna(0).clip(0,1)\n",
    "\n",
    "grid[\"footfall_x_mex_share\"] = pd.to_numeric(grid[\"footfall_day_avg\"], errors=\"coerce\") * (1 - ratio)\n",
    "\n",
    "footfall_norm = normalize_minmax(grid[\"footfall_day_avg\"]).fillna(0)\n",
    "access_norm   = normalize_minmax(grid[\"intermodal_score\"]).fillna(0)\n",
    "compet_norm   = normalize_minmax(1/(1+grid[\"rest_density_500m\"])).fillna(0)\n",
    "mx_aff_norm   = normalize_minmax(grid[\"mex_affinity_proxy\"]).fillna(0)\n",
    "\n",
    "grid[\"mexican_location_score\"] = (\n",
    "    0.35*footfall_norm +\n",
    "    0.25*access_norm +\n",
    "    0.20*compet_norm +\n",
    "    0.20*mx_aff_norm\n",
    ").fillna(0).clip(0,1)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# SANITY CHECKS\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n--- SANITY CHECKS ---\")\n",
    "print(\"footfall_day_avg describe:\\n\", pd.to_numeric(grid[\"footfall_day_avg\"], errors=\"coerce\").describe())\n",
    "print(\"intermodal_score describe:\\n\", pd.to_numeric(grid[\"intermodal_score\"], errors=\"coerce\").describe())\n",
    "print(\"mex_affinity_proxy describe:\\n\", pd.to_numeric(grid[\"mex_affinity_proxy\"], errors=\"coerce\").describe())\n",
    "print(\"mexican_location_score describe:\\n\", pd.to_numeric(grid[\"mexican_location_score\"], errors=\"coerce\").describe())\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# EXPORT\n",
    "# --------------------------------------------------------------\n",
    "cols_out = [\n",
    "    \"grid_id\",\"lat_center\",\"lon_center\",\n",
    "    \"footfall_day_avg\",\n",
    "    \"rest_total_250m\",\"rest_total_500m\",\"rest_density_500m\",\n",
    "    \"competition_raw\",\n",
    "    \"dist_metro_min\",\"estaciones_500m\",\n",
    "    \"dist_parking_min\",\"plazas_parking_500m\",\"intermodal_score\",\n",
    "    \"num_personas\",\"res_density\",\n",
    "    \"mex_affinity_proxy\",\"mx_rest_total_500m\",\n",
    "    \"footfall_x_mex_share\",\n",
    "    \"mexican_location_score\",\n",
    "]\n",
    "\n",
    "grid[cols_out].to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nâœ… ETL V4.3 exportado â†’ {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e97eb69-0d94-4a84-97a6-e9d2669bc3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
